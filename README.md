# ğŸ”¥ Dags-Airflow ğŸ”¥

Este repositÃ³rio contÃ©m a implementaÃ§Ã£o de pipelines de dados utilizando o **Apache Airflow** em um ambiente Docker, seguindo a arquitetura Medallion (Bronze, Silver, Gold). Esse projeto foi desenvolvido para demonstrar como criar e gerenciar pipelines ETL de forma eficiente, escalÃ¡vel e organizada com o Airflow!

![Airflow Logo](https://upload.wikimedia.org/wikipedia/commons/thumb/d/de/Apache-airflow-logo.png/1920px-Apache-airflow-logo.png)

---

##  O que Ã© o Apache Airflow?

O Apache Airflow Ã© uma plataforma open-source usada para orquestrar workflows. O Airflow utiliza DAGs (Directed Acyclic Graphs) que definem o fluxo de execuÃ§Ã£o das tarefas, permitindo o gerenciamento de pipelines complexos com facilidade.

### Principais recursos:
- **Escalabilidade**: OrquestraÃ§Ã£o de pipelines em grande escala.
- **Agendador poderoso**: AutomatizaÃ§Ã£o de workflows atravÃ©s de trigger.
- **Interatividade**: Interface web para monitoramento, logs e execuÃ§Ã£o de tarefas.
  
---

## âš™ï¸ O que sÃ£o DAGs?

Cada DAG representa um pipeline de tarefas. As tarefas dentro de uma DAG sÃ£o executadas em uma ordem especÃ­fica, dependendo das suas dependÃªncias e configuraÃ§Ãµes de agendamento.

Nesse projeto, as DAGs orquestram tarefas ETL para cada camada da arquitetura.

---

## ğŸ›ï¸ Arquitetura Medallion (Bronze, Silver, Gold):

O modelo Medallion Ã© uma arquitetura comum usada em pipelines de dados. Ele separa os dados em diferentes camadas, promovendo a limpeza e transformaÃ§Ã£o progressiva conforme os dados avanÃ§am de uma camada para outra.

- **Bronze**: Camada bruta de dados, onde sÃ£o armazenados os dados nÃ£o processados.
- **Silver**: Camada intermediÃ¡ria de dados, onde os dados sÃ£o limpos e transformados.
- **Gold**: Camada de dados refinados, prontos para consumo para criaÃ§Ã£o de ML e dashboards.

---

## ğŸ› ï¸ Estrutura do Projeto:

```bash
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ dag_bronze.py   
â”‚   â””â”€â”€ dag_silver.py   
â”‚   â””â”€â”€ dag_gold.py   
â”œâ”€â”€ ETL/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â””â”€â”€ main.py  # Classe ETL   
â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â””â”€â”€ main.py   # Classe ETL  
â”‚   â”œâ”€â”€ gold/
â”‚      â””â”€â”€ main.py    # Classe ETL  
â”‚
â”œâ”€â”€ data/ # DiretÃ³rio de armazenamento dos dados.
â”‚   â”œâ”€â”€ d01_raw/
â”‚   â”œâ”€â”€ d02_bronze/
â”‚   â”œâ”€â”€ d03_silver/
â”‚   â”œâ”€â”€ d04_gold/
â”‚
â”œâ”€â”€ docker-compose.yaml # Arquivo de configuraÃ§Ã£o do Docker Compose para o Airflow      
â””â”€â”€ README.md           
```
---

## ğŸ³ Docker Compose:

O projeto utiliza o Docker Compose para orquestrar os containers necessÃ¡rios para executar o Airflow localmente. O arquivo docker-compose.yaml define a infraestrutura mÃ­nima necessÃ¡ria, contendo os serviÃ§os do scheduler, webserver, e Postgres como banco de dados backend.


- **Scheduler**: ResponsÃ¡vel pelo processo de execuÃ§Ã£o das DAGs.
- **Webserver**: Interface web para monitoramento e controle.
- **Postgres**: Banco de dados para armazenamento de metadados.
- **Volumes**: ConfiguraÃ§Ã£o de volumes para persistÃªncia de dados, importaÃ§Ã£o de mÃ³dulos entre arquivos e logs.

### Exemplo de execuÃ§Ã£o:

```bash
docker-compose up --build
```

IrÃ¡ subir todos os serviÃ§os nescessÃ¡rios... (Imagens, Containers, Volumes)

---

## Requisitos Projeto:

### 1.PrÃ©-requisitos:

- **Docker**: InstalaÃ§Ã£o do Docker.

### 2.ExecuÃ§Ã£o do Projeto:

**Clone:**

```bash
git clone https://github.com/ViniciusOtoni/Dags-Airflow.git
```

**Subir serviÃ§os**
```bash
docker-compose up --build
```

Acesse o Airflow na porta 8080 localmente.